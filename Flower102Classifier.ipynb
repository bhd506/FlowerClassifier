{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obd7eBTQXhPa"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import scipy.io as scp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import skimage.io as skio\n",
        "import scipy.io as scp\n",
        "from torch.utils.data import Dataset, DataLoader, Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCZ8TtYqXs05",
        "outputId": "b5e2133c-786e-4047-95e8-3f33ab23d258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ywRe521JMUJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC_qpXlcYT3M"
      },
      "outputs": [],
      "source": [
        "class MyCNN(nn.Module):\n",
        "  def __init__(self, num_channels, num_out_ch, img_w, img_h, num_classes):\n",
        "    super(MyCNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=num_out_ch[0],\n",
        "                           kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
        "    self.bn1 = nn.BatchNorm2d(num_out_ch[0])\n",
        "    self.conv2 = nn.Conv2d(in_channels=num_out_ch[0], out_channels=num_out_ch[1],\n",
        "                           kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
        "    self.bn2 = nn.BatchNorm2d(num_out_ch[1])\n",
        "    self.conv3 = nn.Conv2d(in_channels=num_out_ch[1], out_channels=num_out_ch[1],\n",
        "                           kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
        "    self.bn3 = nn.BatchNorm2d(num_out_ch[1])\n",
        "    self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "    self.fc = nn.Linear(in_features = int(img_w//256)*int(img_h//256)*num_out_ch[1], out_features=num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x))) #1\n",
        "    x = self.pool(F.relu(self.conv2(x))) #2\n",
        "    x = self.pool(F.relu(self.conv3(x))) #3\n",
        "    x = self.pool(F.relu(self.conv3(x))) #4\n",
        "    x = self.pool(F.relu(self.conv3(x))) #5\n",
        "    x = self.pool(F.relu(self.conv3(x))) #6\n",
        "    x = self.pool(F.relu(self.conv3(x))) #7\n",
        "    x = self.pool(F.relu(self.conv3(x))) #8\n",
        "    x = self.fc(x.reshape(x.shape[0], -1))\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Define the convolutional blocks\n",
        "        self.block1 = ConvBlock(input_channels, 32)\n",
        "        self.block2 = ConvBlock(32, 32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.block3 = ConvBlock(32, 64)\n",
        "        self.block4 = ConvBlock(64, 64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.block5 = ConvBlock(64, 128)\n",
        "        self.block6 = ConvBlock(128, 128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.block7 = ConvBlock(128, 256)\n",
        "        self.block8 = ConvBlock(256, 256)\n",
        "\n",
        "        self.block9 = ConvBlock(256, 512)\n",
        "        self.block10 = ConvBlock(512, 512)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.block11 = ConvBlock(512, 512)\n",
        "        self.block12 = ConvBlock(512, 512)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calculate the size of the input to the fully connected layer\n",
        "        self.fc_input_size = self._get_fc_input_size(input_channels)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.fc_input_size, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def _get_fc_input_size(self, input_channels):\n",
        "        # Create a dummy input to pass through the convolutional layers to determine the size\n",
        "        dummy_input = torch.zeros(1, input_channels, 64, 64)  # Assuming input image size is 64x64\n",
        "        with torch.no_grad():\n",
        "            dummy_output = self.block1(dummy_input)\n",
        "            dummy_output = self.block2(dummy_output)\n",
        "            dummy_output = self.pool1(dummy_output)\n",
        "            dummy_output = self.block3(dummy_output)\n",
        "            dummy_output = self.block4(dummy_output)\n",
        "            dummy_output = self.pool2(dummy_output)\n",
        "            dummy_output = self.block5(dummy_output)\n",
        "            dummy_output = self.block6(dummy_output)\n",
        "            dummy_output = self.pool3(dummy_output)\n",
        "            dummy_output = self.block7(dummy_output)\n",
        "            dummy_output = self.block8(dummy_output)\n",
        "            dummy_output = self.block9(dummy_output)\n",
        "            dummy_output = self.block10(dummy_output)\n",
        "            dummy_output = self.pool4(dummy_output)\n",
        "            dummy_output = self.block11(dummy_output)\n",
        "            dummy_output = self.block12(dummy_output)\n",
        "            dummy_output = self.pool5(dummy_output)\n",
        "        return dummy_output.view(-1).size(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.block7(x)\n",
        "        x = self.block8(x)\n",
        "\n",
        "        x = self.block9(x)\n",
        "        x = self.block10(x)\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        x = self.block11(x)\n",
        "        x = self.block12(x)\n",
        "        x = self.pool5(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "mB4qzDqqMZwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zSk60TWY3Bf"
      },
      "outputs": [],
      "source": [
        "NUM_OUT_CH = [8, 16]\n",
        "IMAGE_W = 256\n",
        "IMAGE_H = 256\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 12\n",
        "LR = 0.0001\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "input_channels = 3\n",
        "num_classes = 10\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "    # Create the model\n",
        "#model = SimpleCNN(input_channels, num_classes)\n",
        "\n",
        "# model\n",
        "model = MyCNN(num_channels=3, num_out_ch=NUM_OUT_CH, img_w=IMAGE_W, img_h=IMAGE_H, num_classes=102)\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = LR)\n",
        "\n",
        "# Loss Function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uq-ObF9ZIPI"
      },
      "outputs": [],
      "source": [
        "flower_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_W, IMAGE_H)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "augmented_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_W, IMAGE_H)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Flip the images randomly with a probability of 0.5\n",
        "    transforms.RandomRotation(15),  # Randomly rotate images in the range (-15, 15) degrees\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Randomly change brightness and contrast\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "root = '/content/drive/MyDrive/ActualFlowers/jpg'\n",
        "\n",
        "train_set = torchvision.datasets.Flowers102(root = root, split = 'train', transform = augmented_transform, target_transform = None, download = False)\n",
        "test_set = torchvision.datasets.Flowers102(root = root, split = 'test', transform = flower_transform, target_transform = None, download = False)\n",
        "validation_set = torchvision.datasets.Flowers102(root = root, split = 'val', transform = flower_transform, target_transform = None, download = False)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validation_loader = DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0qmb50uY8ZA"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model, num_classes=102):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Initialize the confusion matrix\n",
        "    confusion_matrix = torch.zeros(num_classes, num_classes, dtype=torch.int64)\n",
        "\n",
        "    with torch.no_grad():  # Do not calculate gradients\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)  # Move data to the device\n",
        "            y = y.to(device)  # Move labels to the device\n",
        "            scores = model(x)  # Compute model output\n",
        "            _, predictions = scores.max(1)  # Get the predicted classes\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "            # Update confusion matrix\n",
        "            for t, p in zip(y.view(-1), predictions.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "    model.train()  # Set the model back to training mode\n",
        "    accuracy = float(num_correct) / num_samples  # Calculate accuracy\n",
        "\n",
        "    # Print overall accuracy\n",
        "    print(f\"Got {num_correct} / {num_samples} with accuracy {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Print confusion matrix or other statistics if necessary\n",
        "    # For detailed analysis, you might return or further process the confusion matrix\n",
        "    return accuracy, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uln6No72ZSuu"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct_predictions / len(loader.dataset)\n",
        "    model.train()  # Set the model back to training mode\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "balxC9BhZWrr",
        "outputId": "9f13e6bc-00e5-41b5-d332-e920880aef64"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [03:52<00:00, 14.54s/batch, loss=4.65]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Training loss: 74.1755\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:51<00:00,  3.23s/batch, loss=4.61]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Training loss: 74.1708\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:52<00:00,  3.26s/batch, loss=4.63]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Training loss: 74.1706\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [01:01<00:00,  3.85s/batch, loss=4.66]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Training loss: 74.1714\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:53<00:00,  3.32s/batch, loss=4.64]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Training loss: 74.1686\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:52<00:00,  3.31s/batch, loss=4.6]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Training loss: 74.1652\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:52<00:00,  3.31s/batch, loss=4.66]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Training loss: 74.1676\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:51<00:00,  3.24s/batch, loss=4.63]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Training loss: 74.1645\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:52<00:00,  3.30s/batch, loss=4.67]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Training loss: 74.1660\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:53<00:00,  3.31s/batch, loss=4.63]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Training loss: 74.1625\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "n_epochs_stop = 10\n",
        "for epoch in range(NUM_EPOCHS*5):\n",
        "    running_loss = 0\n",
        "    with tqdm.tqdm(train_loader, unit='batch') as tepoch:\n",
        "        for index, (x, y) in enumerate(tepoch):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            y_hat = model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "    # Compute average training loss\n",
        "    avg_training_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch}: Training loss: {running_loss:.4f}\")\n",
        "    if (epoch+1)%10==0:\n",
        "     check_accuracy(test_loader, model)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    if epoch >= 10:\n",
        "      validation_loss, validation_accuracy = evaluate(model, validation_loader, device)\n",
        "      print(f\"Epoch {epoch}: Validation loss: {validation_loss:.4f}, Validation accuracy: {validation_accuracy*100:.4f}%\")\n",
        "\n",
        "      # Check for early stopping\n",
        "      if validation_loss < best_val_loss:\n",
        "          best_val_loss = validation_loss\n",
        "          epochs_no_improve = 0\n",
        "          # Save the model if validation loss improves\n",
        "          torch.save(model.state_dict(), 'best_model_v1.pth')\n",
        "      else:\n",
        "          epochs_no_improve += 1\n",
        "          print(f\"No improvement in validation loss for {epochs_no_improve} epochs.\")\n",
        "\n",
        "      # Early stopping condition\n",
        "      if epochs_no_improve == n_epochs_stop:\n",
        "          print(\"Early stopping triggered\")\n",
        "          break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}